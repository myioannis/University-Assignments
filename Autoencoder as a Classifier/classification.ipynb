{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "64KBwCwdR__9"
      },
      "source": [
        "import sys, os\n",
        "# If this is run on colab, it clones the git repository, so that you don't have to upload the datasets on your google drive\n",
        "if 'google.colab' in sys.modules:\n",
        "  !git clone https://github.com/myioannis/Project-2.git\n",
        "  # Change to the directory of the cloned repository\n",
        "  %cd Project-2\n",
        "  sys.path.append(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arxN7GOXSUsM"
      },
      "source": [
        "from utilities import read_Data, read_Labels, AE_parse_CLA, CL_parse_CLA, AE_read_Hyperparameters, CL_read_Hyperparameters, preprocess #from our utilities.py\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrcKlsT0SUp1"
      },
      "source": [
        "def classification(encoder,fc_nodes):\n",
        "    # Flatten\n",
        "    flat = Flatten()(encoder)\n",
        "    # Fully connected\n",
        "    dense = Dense(fc_nodes, activation='relu')(flat)\n",
        "    # Dropout\n",
        "    drop = Dropout(0.7)(dense)\n",
        "    # Output\n",
        "    classified = Dense(10, activation='softmax')(drop)\n",
        "    return classified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqHhivmZSUnV"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Parse the command line arguments\n",
        "  trainData_Path, testData_Path, trainLabels_Path, testLabels_Path, model_Path = CL_parse_CLA(sys.argv)\n",
        "  # If any of the path arguments was not given (forgotten or running as a jupyter notebook), then ask for them\n",
        "  if not trainData_Path: trainData_Path = input('Please provide the path of the training data:') # trainData_Path = \"train-images-idx3-ubyte.gz\"\n",
        "  if not testData_Path: testData_Path = input('Please provide the path of the test data:') # testData_Path = \"t10k-images-idx3-ubyte.gz\"\n",
        "  if not trainLabels_Path: trainLabels_Path = input('Please provide the path of the training labels:') # trainLabels_Path = \"train-labels-idx1-ubyte.gz\"\n",
        "  if not testLabels_Path: testLabels_Path = input('Please provide the path of the test labels:') # testLabels_Path = \"t10k-labels-idx1-ubyte.gz\"\n",
        "  if not model_Path: model_Path = input('Please provide the path of the autoencoder model:') # model_Path = \"autoencoder_best.h5\"\n",
        "  # Read the all the datasets\n",
        "  trainData = read_Data(trainData_Path)\n",
        "  testData = read_Data(testData_Path)\n",
        "  trainLabels = read_Labels(trainLabels_Path)\n",
        "  testLabels = read_Labels(testLabels_Path)\n",
        "  # Load the autoencoder model and keep its encoding part\n",
        "  autoencoder = load_model(model_Path)\n",
        "  autoencoder_length = len(autoencoder.layers)\n",
        "  encoder_length = int(autoencoder_length/2) -1\n",
        "  encoder = autoencoder.layers[encoder_length].output\n",
        "  # Preprocess the data\n",
        "  trainData = preprocess(trainData)\n",
        "  testData = preprocess(testData)\n",
        "\n",
        "  history = [] # a list of the trained models' loss history, the corresponding hyperparameters of each model and the models themselves\n",
        "  choice = 1\n",
        "  while choice != 0 :\n",
        "    if choice == 1:\n",
        "      # Read the hyperparameters\n",
        "      hyperparameters = CL_read_Hyperparameters()\n",
        "      fc_nodes, epochs, batch_size = hyperparameters\n",
        "      # Create the full model\n",
        "      full_model = Model(autoencoder.input,classification(encoder,fc_nodes))\n",
        "      # Set the weights of the encoding part of the full model to be the weights of the encoding part of the autoencoder model we loaded\n",
        "      for layer_full,layer_encoder in zip(full_model.layers[:encoder_length],autoencoder.layers[:encoder_length]):\n",
        "          layer_full.set_weights(layer_encoder.get_weights())\n",
        "      # Set the layers of the encoding part of the full model to non-trainable\n",
        "      for layer in full_model.layers[0:encoder_length]:\n",
        "          layer.trainable = False\n",
        "      full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=RMSprop(lr=0.001),metrics=['accuracy'])\n",
        "      # Train the non-encoding part of the full model\n",
        "      classify_train = full_model.fit(\n",
        "          trainData, \n",
        "          to_categorical(trainLabels), \n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          validation_split=0.2,\n",
        "          shuffle=True\n",
        "      )\n",
        "      # Set back to trainable the layers of the encoding part of the full model \n",
        "      for layer in full_model.layers[0:encoder_length]:\n",
        "          layer.trainable = True\n",
        "      full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=RMSprop(lr=0.001), metrics=['accuracy'])\n",
        "      # Train the whole full model\n",
        "      classify_train = full_model.fit(\n",
        "          trainData, \n",
        "          to_categorical(trainLabels), \n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_split=0.2,\n",
        "          shuffle=True\n",
        "      )\n",
        "      history.append((classify_train,hyperparameters,full_model))\n",
        "    elif choice == 2:\n",
        "      # Plot each model's training and validation loss and accuracy\n",
        "      for index,triple in enumerate(history):\n",
        "        classify_train, hyperparams, model = triple\n",
        "        fc_nodes, epochs, batch_size = hyperparams\n",
        "        print(f'Model {index}: Nodes of Fully Connected Layer = {fc_nodes}, Epochs = {epochs}, Batch Size = {batch_size}')\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(classify_train.history['loss'])\n",
        "        plt.plot(classify_train.history['val_loss'])\n",
        "        plt.title('Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(classify_train.history['accuracy'])\n",
        "        plt.plot(classify_train.history['val_accuracy'])\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.show()        \n",
        "    elif choice == 3:\n",
        "      # Classify the test data according to one of the models and print some statistics\n",
        "      model_num = int(input(f\"Which model from {0} to {len(history)-1} would you like to use?:\"))\n",
        "      test_eval = history[model_num][2].evaluate(testData, to_categorical(testLabels), verbose=0)\n",
        "      print('\\nTest loss:', test_eval[0])\n",
        "      print('Test accuracy:', test_eval[1])\n",
        "      # Print the number of correctly and incorrect classified images, and a sample of each\n",
        "      predicted_classes = history[model_num][2].predict(testData)\n",
        "      predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "      correct = np.where(predicted_classes==testLabels)[0]\n",
        "      incorrect = np.where(predicted_classes!=testLabels)[0]\n",
        "      print(f\"\\nFound -{len(correct)}- CORRECT labels. Some of them are:\")\n",
        "      for i, correct in enumerate(correct[:9]):\n",
        "          plt.subplot(3,3,i+1)\n",
        "          plt.imshow(testData[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
        "          plt.title(f\"Predicted {predicted_classes[correct]}, Actual {testLabels[correct]}\")\n",
        "          plt.tight_layout()\n",
        "      plt.show()\n",
        "      print(f\"\\nFound {len(incorrect)} INCORRECT labels. Some of them are:\")\n",
        "      for i, incorrect in enumerate(incorrect[:9]):\n",
        "          plt.subplot(3,3,i+1)\n",
        "          plt.imshow(testData[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
        "          plt.title(f\"Predicted {predicted_classes[incorrect]}, Actual {testLabels[incorrect]}\")\n",
        "          plt.tight_layout()\n",
        "      plt.show()\n",
        "      # Print the classification report (f-score etc.)\n",
        "      target_names = [f\"Class {i}\" for i in range(10)]\n",
        "      print(classification_report(testLabels, predicted_classes, target_names=target_names))\n",
        "    choice = int(input(\"What would you like to do next?\\n 0) Exit\\n 1) Repeat the experiment with different parameters\\n 2) Plot the metrics for each experiment\\n 3) Classify the images in the test set\\n\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAcaHOwkSQkG"
      },
      "source": [
        "# This deletes the cloned repository from the current colab session. You don't have to run it, since when the session ends, all files are deleted\n",
        "if 'google.colab' in sys.modules:\n",
        "  %cd ..\n",
        "  !rm -rf Project-2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}