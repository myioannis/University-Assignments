{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "reduce.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "49I5xMy1Rh6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56aaddc2-1c40-4d0a-d64c-db1fddc9b27d"
      },
      "source": [
        "import sys, os\n",
        "# If this is run on colab, it clones the git repository, so that you don't have to upload the datasets on your google drive\n",
        "if 'google.colab' in sys.modules:\n",
        "  !git clone https://github.com/myioannis/Project-3.git\n",
        "  # Change to the directory of the cloned repository\n",
        "  %cd Project-3\n",
        "  sys.path.append(os.getcwd())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Project-3'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 127 (delta 50), reused 90 (delta 34), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (127/127), 29.70 MiB | 31.88 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n",
            "/content/Project-3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHL-Iz4GknTs"
      },
      "source": [
        "from utilities import read_Data, read_Labels, reduce_parse_CLA, AE_read_Hyperparameters, preprocess #from our utilities.py\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Reshape\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from math import ceil\n",
        "import struct\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBgTmWGQkofj"
      },
      "source": [
        "strides = (2,2)\n",
        "\n",
        "def conv_output_shape(in_height,in_width,strides):\n",
        "  \"\"\" Returns the expected output height and width of the --conv2D-- layer after --downsampling-- with strides \"\"\"\n",
        "  out_height = ceil(float(in_height) / strides[0])\n",
        "  out_width  = ceil(float(in_width) / strides[1])\n",
        "  return (out_height,out_width)\n",
        "\n",
        "def trans_output_shape(in_height,in_width,strides):\n",
        "  \"\"\" Returns the expected output height and width of the --conv2DTranspose-- layer after --upsampling-- with strides \"\"\"\n",
        "  out_height = in_height * strides[0]\n",
        "  out_width  = in_width * strides[1]\n",
        "  return (out_height,out_width)\n",
        "\n",
        "def find_padding(in_height,in_width):\n",
        "  \"\"\" Finds the right padding to be forced on the conv2DTranspose to achieve upsampling symmetrical to the conv2D downsampling \"\"\"\n",
        "  expected_conv_height, expected_conv_width = conv_output_shape(in_height,in_width,strides)\n",
        "  expected_trans_height, expected_trans_width = trans_output_shape(expected_conv_height,expected_conv_width,strides)\n",
        "  return (in_height-expected_trans_height+1,in_width-expected_trans_width+1)\n",
        "\n",
        "def find_encoding_output(model,num_latent_dimensions):\n",
        "  for layer_num,layer in enumerate(model.layers):\n",
        "    if isinstance(layer,keras.layers.Dense) and layer.output_shape[1] == num_latent_dimensions:\n",
        "      return layer.output\n",
        "\n",
        "def encoder(conv, hyperparameters, num_latent_dimensions):\n",
        "  \"\"\" The encoding part of the autoencoder \"\"\"\n",
        "  # Read the hyperparameters\n",
        "  conv_layers, kernel_size, conv_filters = hyperparameters[:3]\n",
        "  # Keep a list of the shape of each encoding layer --after downsampling-- in order to help the decoder do a --symmetrical upsampling--\n",
        "  encoding_shapes = []\n",
        "  # First convolutional layers\n",
        "  for i in range(conv_layers-1):\n",
        "      conv = BatchNormalization()(conv)\n",
        "      out_height,out_width = conv_output_shape(conv.shape[1],conv.shape[2],strides)\n",
        "      if out_height > 0 and out_width > 0:\n",
        "        conv = Conv2D(conv_filters[i], (kernel_size, kernel_size),  activation='relu', strides=strides, padding='same')(conv)\n",
        "        encoding_shapes.append((out_height,out_width))\n",
        "        conv = Dropout(0.3)(conv)\n",
        "      else:\n",
        "        # Do not downsample (no strides)\n",
        "        conv = Conv2D(conv_filters[i], (kernel_size, kernel_size), activation='relu', padding='same')(conv)\n",
        "\n",
        "  # Last convolutional layer\n",
        "  out_height,out_width = conv_output_shape(conv.shape[1],conv.shape[2],strides)\n",
        "  if out_height > 0 and out_width > 0:\n",
        "    conv = Conv2D(conv_filters[conv_layers-1], (kernel_size, kernel_size),  activation='relu', strides=strides, padding='same')(conv)\n",
        "    encoding_shapes.append((out_height,out_width))\n",
        "  else:\n",
        "    # Do not downsample (no strides)\n",
        "    conv = Conv2D(conv_filters[conv_layers-1], (kernel_size, kernel_size), activation='relu', padding='same')(conv)\n",
        "\n",
        "  conv = Flatten()(conv)\n",
        "  conv = Dense(num_latent_dimensions, activation='relu')(conv)\n",
        "  return conv,tuple(encoding_shapes)\n",
        "\n",
        "\n",
        "\n",
        "def decoder(encoder, hyperparameters):\n",
        "  \"\"\" The decoding part of the autoencoder \"\"\"\n",
        "  conv_layers, kernel_size, conv_filters = hyperparameters[:3]\n",
        "  conv, encoding_shapes = encoder\n",
        "\n",
        "  in_height, in_width = encoding_shapes[-1]\n",
        "  print(encoding_shapes)\n",
        "  conv = Dense(in_height*in_width*conv_filters[-1], activation='relu')(conv)\n",
        "  conv = Reshape((in_height,in_width,conv_filters[-1]))(conv)\n",
        "\n",
        "  # First convolutional layers\n",
        "  for i in reversed(range(conv_layers-1)):\n",
        "      conv = BatchNormalization()(conv)\n",
        "      if i <= len(encoding_shapes)-1:\n",
        "        in_height, in_width = encoding_shapes[i]\n",
        "        conv = Conv2DTranspose(conv_filters[i], (kernel_size, kernel_size), strides=strides, output_padding=find_padding(in_height,in_width), padding='same', activation='relu')(conv)\n",
        "        conv = Dropout(0.3)(conv)\n",
        "      else:\n",
        "        # Do not upsample\n",
        "        conv = Conv2DTranspose(conv_filters[i], (kernel_size, kernel_size), activation='relu')(conv)\n",
        "\n",
        "  # Last convolutional layer\n",
        "  decoded = Conv2DTranspose(1, (kernel_size, kernel_size), strides=strides, padding='same', activation='sigmoid')(conv)\n",
        "  return decoded\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMfjuSHBJ_HV"
      },
      "source": [
        "def init_output_file(filename, numOfImages, numOfRows, numOfColumns):\n",
        "  \"\"\" Initializes the output file with magicNumber, numOfImages, numOfRows, numOfColumns \"\"\"\n",
        "  with open(filename, 'wb') as file:\n",
        "    file.write(struct.pack('>i', 42))\n",
        "    file.write(struct.pack('>i', numOfImages))\n",
        "    file.write(struct.pack('>i', numOfRows))\n",
        "    file.write(struct.pack('>i', numOfColumns))\n",
        "\n",
        "def write_to_output(filename, vector):\n",
        "  \"\"\" Writes latent representation of an image in the output file (high-endian) \"\"\"\n",
        "  with open(filename, 'ab') as file:\n",
        "    for pixel in vector:\n",
        "      file.write(struct.pack('>H', pixel))\n",
        "\n",
        "def latent_dimensions(encoder_model):\n",
        "  \"\"\" Returns the number of latent dimensions of a --pretrained-- model \"\"\"\n",
        "  return encoder_model.layers[-1].output_shape[1]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcGvvd4U8yAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0787d936-392c-422c-8025-56444420cad4"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Parse the command line arguments\n",
        "  trainData_Path, testData_Path, trainOutput_Path, testOutput_Path = reduce_parse_CLA(sys.argv)\n",
        "  # If any of the path arguments was not given (forgotten or running as a jupyter notebook), then ask for them\n",
        "  if not trainData_Path: trainData_Path = input('Please provide the path of the training data: ')\n",
        "  if not testData_Path: testData_Path = input('Please provide the path of the test data: ')\n",
        "\n",
        "  # Read the datasets\n",
        "  trainData = read_Data(trainData_Path) #train-images-idx3-ubyte.gz\n",
        "  testData = read_Data(testData_Path) #t10k-images-idx3-ubyte.gz\n",
        "\n",
        "  # Preprocess the data\n",
        "  trainData = preprocess(trainData)\n",
        "  testData = preprocess(testData)\n",
        "\n",
        "  history = [] # a list of the trained models' loss history, the corresponding hyperparameters of each model and the models themselves\n",
        "  choice = \"start\"\n",
        "  num_latent_dimensions = 0\n",
        "  while choice != 0 :\n",
        "    if choice == \"start\":\n",
        "      inner_choice = int(input(\"What would you like to do?\\n 0) Exit\\n 1) Upload a pretrained --encoder-- model and convert the images to their latent representation\\n 2) Train your own model\\n\"))\n",
        "      \n",
        "      if inner_choice == 1:\n",
        "        # Upload a pretrained --encoder-- model\n",
        "        model_Path = input('Please provide the path of the --encoder-- model: ')\n",
        "        encoder_model = load_model(model_Path)\n",
        "\n",
        "        # Predict (convert into latent representations) on the data and query sets\n",
        "        encoded_data = encoder_model.predict(trainData)\n",
        "        encoded_queries = encoder_model.predict(testData)\n",
        "\n",
        "        print(\"latent_dimensions: \",latent_dimensions(encoder_model))\n",
        "        # Initialize the output files\n",
        "        init_output_file(trainOutput_Path, len(trainData), 1, latent_dimensions(encoder_model))\n",
        "        init_output_file(testOutput_Path, len(testData), 1, latent_dimensions(encoder_model))\n",
        "\n",
        "        # Write the latent representations of the -data- into the output file\n",
        "        encoded_max = np.amax(encoded_data)\n",
        "        print(encoded_max)\n",
        "        for i in range(len(trainData)):\n",
        "          encoded_img = encoded_data[i]\n",
        "          encoded_img *= 25500.0/encoded_max\n",
        "          encoded_img = encoded_img.astype(int)\n",
        "          write_to_output(trainOutput_Path, encoded_img)\n",
        "        \n",
        "        # Write the latent representations of the -queries- into the output file\n",
        "        encoded_max = np.amax(encoded_queries)\n",
        "        print(encoded_max)\n",
        "        for i in range(len(testData)):\n",
        "          encoded_img = encoded_queries[i]\n",
        "          encoded_img *= 25500.0/encoded_max\n",
        "          encoded_img = encoded_img.astype(int)\n",
        "          write_to_output(testOutput_Path, encoded_img)\n",
        "  \n",
        "      elif inner_choice == 2:\n",
        "        choice = 1\n",
        "        continue\n",
        "\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    elif choice == 1:\n",
        "      # Ask the user for the hyperparameters\n",
        "      hyperparameters = AE_read_Hyperparameters()\n",
        "      epochs, batch_size = hyperparameters[3:]\n",
        "      num_latent_dimensions = int(input(\"How many dimensions would you like the latent representations of the images to be? \\n\"))\n",
        "      \n",
        "      # Create the model\n",
        "      input_img = Input(shape = (trainData.shape[1], trainData.shape[2], 1))\n",
        "      autoencoder = Model(input_img, decoder(encoder(input_img,hyperparameters,num_latent_dimensions),hyperparameters))\n",
        "      print(autoencoder.summary())\n",
        "      autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop(lr=0.001))\n",
        "      # Train the model\n",
        "      autoencoder_train = autoencoder.fit(\n",
        "          trainData, \n",
        "          trainData, \n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_split=0.2,\n",
        "          shuffle=True\n",
        "      )\n",
        "      history.append((autoencoder_train,hyperparameters,autoencoder))\n",
        "\n",
        "    elif choice == 2:\n",
        "      # Plot the loss and validation loss of each model\n",
        "      for index,triple in enumerate(history):\n",
        "        autoencoder_train, hyperparams, model = triple\n",
        "        conv_layers, kernel_size, conv_filters, epochs, batch_size = hyperparams\n",
        "        print(f'Model {index}: Convolutional Layers = {conv_layers}, Kernel Size = {kernel_size}, Convolutional Filters = {conv_filters}, Epochs = {epochs}, Batch Size = {batch_size}')\n",
        "        plt.plot(autoencoder_train.history['loss'])\n",
        "        plt.plot(autoencoder_train.history['val_loss'])\n",
        "        plt.title('Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.show()        \n",
        "\n",
        "    elif choice == 3:\n",
        "      # Save the --encoding-- part of one of the models\n",
        "      model_num = int(input(f\"Which model from {0} to {len(history)-1} would you like to save?:\"))\n",
        "      model = history[model_num][2]\n",
        "      encoder_output = find_encoding_output(model,num_latent_dimensions)\n",
        "      encoder_model = Model(model.input,encoder_output)\n",
        "      model_Path = input(\"Where would you like the encoder to be saved? Provide relative path: \")\n",
        "      encoder_model.save(model_Path)\n",
        "\n",
        "    elif choice == 4:\n",
        "      model_num = int(input(f\"Which model from {0} to {len(history)-1} would you like to use?:\"))\n",
        "      model = history[model_num][2]\n",
        "      encoder_output = find_encoding_output(model,num_latent_dimensions)\n",
        "      encoder_model = Model(model.input,encoder_output)\n",
        "\n",
        "      # Predict (convert into latent representations) on the data and query sets\n",
        "      encoded_data = encoder_model.predict(trainData)\n",
        "      encoded_queries = encoder_model.predict(testData)\n",
        "\n",
        "      # Initialize the output files\n",
        "      init_output_file(trainOutput_Path, len(trainData), 1, num_latent_dimensions)\n",
        "      init_output_file(testOutput_Path, len(testData), 1, num_latent_dimensions)\n",
        "\n",
        "      # Write the latent representations of the -data- into the output file\n",
        "      encoded_max = np.amax(encoded_data)\n",
        "      print(encoded_max)\n",
        "      for i in range(len(trainData)):\n",
        "        encoded_img = encoded_data[i]\n",
        "        encoded_img *= 25500.0/encoded_max\n",
        "        encoded_img = encoded_img.astype(int)\n",
        "        write_to_output(trainOutput_Path, encoded_img)\n",
        "      \n",
        "      # Write the latent representations of the -queries- into the output file\n",
        "\n",
        "      encoded_max = np.amax(encoded_queries)\n",
        "      print(encoded_max)\n",
        "      for i in range(len(testData)):\n",
        "        encoded_img = encoded_queries[i]\n",
        "        encoded_img *= 25500.0/encoded_max\n",
        "        encoded_img = encoded_img.astype(int)\n",
        "        write_to_output(testOutput_Path, encoded_img)\n",
        "\n",
        "    choice = int(input(\"What would you like to do next?\\n 0) Exit\\n 1) Train with different parameters\\n 2) Plot the loss for each model\\n 3) Save the --encoding-- part of one of the models\\n 4) Convert the images into their latent representations using one of the trained models\\n\"))\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What would you like to do?\n",
            " 0) Exit\n",
            " 1) Upload a pretrained --encoder-- model and convert the images to their latent representation\n",
            " 2) Train your own model\n",
            "1\n",
            "Please provide the path of the --encoder-- model: encoder_3_3_64_128_256_30_64_100.h5\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "latent_dimensions:  100\n",
            "6388.56\n",
            "6250.8\n",
            "What would you like to do next?\n",
            " 0) Exit\n",
            " 1) Train with different parameters\n",
            " 2) Plot the loss for each model\n",
            " 3) Save the --encoding-- part of one of the models\n",
            " 4) Convert the images into their latent representations using one of the trained models\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMouJ4wuqv_1"
      },
      "source": [
        "# This deletes the cloned repository from the current colab session. You don't have to run it, since when the session ends, all files are deleted\n",
        "if 'google.colab' in sys.modules:\n",
        "  %cd ..\n",
        "  !rm -rf Project-3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}